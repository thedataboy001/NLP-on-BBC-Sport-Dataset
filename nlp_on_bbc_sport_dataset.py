# -*- coding: utf-8 -*-
"""NLP on BBC Sport Dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hYSaI-Ty2QL551nK958LY4zvexafpKui

# Dataset: BBCSport

All rights, including copyright, in the content of the original articles are owned by the BBC.

Consists of 737 documents from the BBC Sport website corresponding to sports news articles in five topical areas from 2004-2005.

#### Class Labels: 5 (athletics, cricket, football, rugby, tennis)
"""

import zipfile
import os

extract = '/content/drive/MyDrive/Personal Project/Natural Langauge Processing /bbcsport'
path = '/content/drive/MyDrive/Personal Project/Natural Langauge Processing /bbcsport-fulltext.zip'

with zipfile.ZipFile(path, 'r') as k:
  k.extractall(extract)

print('Unzipping Complete')

import os
from pathlib import Path

newpath = '/content/drive/MyDrive/Personal Project/Natural Langauge Processing /bbcsport/bbcsport'

data_dir = Path(newpath)

texts = []
labels = []

for label in os.listdir(data_dir):
  category_dir = data_dir / label
  if category_dir.is_dir():
    for file_path in category_dir.glob("*.txt"):
      with open(file_path, encoding='latin-1') as f:
        text = f.read().strip()
        texts.append(text)
        labels.append(label)

print(f"Loaded {len(texts)} documents.")
print("Sample label:", set(labels))

print(texts[1])

"""## Topic Modelling"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import NMF

tfidf = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')

tm = tfidf.fit_transform(texts)

tm

"""#### Non-Negative Matrix Factorization (NMF)"""

from sklearn.decomposition import NMF

nmf = NMF(n_components=7, random_state=42)
nmf_topics = nmf.fit_transform(tm)

len(tfidf.get_feature_names_out())

len(nmf.components_)

nmf.components_

for index,topic in enumerate(nmf.components_):
  print(f'The top 10 words for topic #{index}')
  print([tfidf.get_feature_names_out()[i] for i in topic.argsort()[-10:]])
  print('*\n')

from collections import defaultdict

category_docs = defaultdict(list)
for text, label in zip(texts, labels):
    category_docs[label].append(text)

def extract_topics(texts, n_topics=5, top_n_words=10):
    vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')
    tfidf = vectorizer.fit_transform(texts)

    nmf = NMF(n_components=n_topics, random_state=42)
    nmf_topics = nmf.fit_transform(tfidf)

    feature_names = vectorizer.get_feature_names_out()
    topics = []
    for topic_idx, topic in enumerate(nmf.components_):
        top_words = [feature_names[i] for i in topic.argsort()[:-top_n_words - 1:-1]]
        topics.append(top_words)
    return topics

list(category_docs)

tennis_topics = extract_topics(category_docs["tennis"], n_topics=2, top_n_words=10)
for i, topic in enumerate(tennis_topics):
    print(f"Tennis Sub-category {i+1}: {', '.join(topic)}")

"""# Named Entity Recongnition"""

from transformers import pipeline

ner = pipeline("ner", model="dbmdz/bert-large-cased-finetuned-conll03-english", aggregation_strategy="simple")

def extract_person_and_job(text):
    entities = ner(text)
    persons = [e for e in entities if e['entity_group'] == 'PER']
    # Simple pattern: Find "Person, Job" in text
    results = []
    for person in persons:
        start, end = person['start'], person['end']
        # Look for ", JOB" after the name, up to 40 chars ahead
        tail = text[end:end+40]
        import re
        match = re.search(r', ([\w\s]+)[\.,]', tail)
        job = match.group(1).strip() if match else None
        results.append({"name": person['word'], "job": job})
    return results

results = [extract_person_and_job(text) for text in category_docs["tennis"]]
print(results[:20])

import spacy
nlp = spacy.load("en_core_web_sm")
def spacy_job_title_extract(text):
    doc = nlp(text)
    results = []
    for ent in doc.ents:
        if ent.label_ == "PERSON":
            for token in ent.root.children:
                if token.dep_ == "appos":
                    results.append({"name": ent.text, "job": token.text})
    return results

results2 = [spacy_job_title_extract(text) for text in category_docs["tennis"]]
print(results2[:20])

results3 = [spacy_job_title_extract(text) for text in texts]
print(results3[:20])

# Text Summarisation Model

summarizer = pipeline("summarization", model="facebook/bart-large-cnn")
sum = summarizer(texts[1], max_length=130, min_length=30, do_sample=False)
print(sum)

print(texts[1])
print('\n')
print(sum)